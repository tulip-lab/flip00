{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (00): Data Science \n",
    "**(Module 03: Linear Algebra)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use,but NOT allowed to change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2022 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "plt.xkcd()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition for Data Visualization\n",
    "\n",
    "## Displaying high-dimensional data using reduced-rank matrices\n",
    "\n",
    "A data visualization goes a long way in helping you understand the underlying dataset. If the data is highly dimensional, you can use Singular Value Decomposition (SVD) to find a reduced-rank approximation of the data that can be visualized easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: the Iris dataset\n",
    "\n",
    "We start off with the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). The data is multivariate, with 150 measurements of 4 features (length and width cm of both sepal and petal) on 3 distinct Iris species. Of the 150 measurements, there are 50 measurements each for _Iris setosa_, _Iris versicolor_, and _Iris virginica_.\n",
    "\n",
    "[Scikit Learn's `datasets`](http://scikit-learn.org/stable/datasets/) includes the Iris dataset, so let's load that up and start exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "print('Iris dataset has {} rows and {} columns\\n'.format(*df_iris.shape))\n",
    "\n",
    "print('Here are the first 5 rows of the data:\\n\\n{}\\n'.format(df_iris.head(5)))\n",
    "\n",
    "print('Some simple statistics on the Iris dataset:\\n\\n{}\\n'.format(df_iris.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are exploring the dataset, it would be nice to view the data in order to get an idea of how the 3 species might be distributed with respect to one another in terms of their features. Perhaps we are interested in finding clusters, or maybe we would like to find a way to make class predictions?\n",
    "\n",
    "However, since the data has 4 dimensions, we would be hard-pressed to come up with a good way to graph the data in 4D that we could easily understand.\n",
    "\n",
    "_But what if we could reduce or compress the data so that we could work in 3 dimensions or less?_\n",
    "\n",
    "[Singular value decomposition](http://mathworld.wolfram.com/SingularValueDecomposition.html) lets us do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "Singular value decomposition factorizes an $\\mathbb{R}^{m \\times n}$ matrix $X$ into\n",
    "\n",
    "* matrix $U \\in \\mathbb{R}^{m \\times m}$ are the left-singular vectors of $X$, where columns are the set of orthonormal eigenvectors of $X \\, X^{\\intercal}$\n",
    "* diagonal matrix $\\Sigma$ with entries $\\sigma \\in \\mathbb{R}$ that are the non-negative singular values of $X$\n",
    "* matrix $V \\in \\mathbb{R}^{n \\times n}$ are the right-singular vectors $X$, where the columns are the set of orthonormal eigenvectors of $X^{\\intercal} \\, X$\n",
    "\n",
    "such that \n",
    "\n",
    "\\begin{align}\n",
    "  X &= U \\, \\Sigma \\, V^{\\intercal}\n",
    "\\end{align}\n",
    "\n",
    "We can use [`numpy.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) to factorize the Iris data matrix into three components $U$, $\\Sigma$, and $V^{\\intercal}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_iris, S_iris, Vt_iris = np.linalg.svd(df_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $U$: left-singular vectors of $X$\n",
    "\n",
    "The rows of the $U$ correspond to the rows of original data matrix $X$, while the columns are the set of ordered, orthornormal eigenvectors of $X \\, X^{\\intercal}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('matrix U has {} rows, {} columns\\n'.format(*U_iris.shape))\n",
    "\n",
    "print('{}'.format(pd.DataFrame(U_iris).head(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $V$: right-singular vectors of $X$\n",
    "\n",
    "[`numpy.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) actually returns $V^{\\intercal}$ instead of $V$, so it is the _columns_ of $V^{\\intercal}$ that correspond to the columns of original data matrix $X$. Hence, the _rows_ are the set of ordered, orthornormal eigenvectors of $X^{\\intercal} \\, X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('matrix Vt has {} rows, {} columns\\n'.format(*Vt_iris.shape))\n",
    "\n",
    "print('{}'.format(pd.DataFrame(Vt_iris).head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\Sigma$: singular values of $X$\n",
    "\n",
    "The elements $\\sigma_{i}$ of diagonal matrix $\\Sigma$ are the non-zero singular values of matrix $X$, which are really just the square roots of the non-zero eigenvalues of $X^{\\intercal} \\, X$ (and also for $X \\, X^{\\intercal}$). These singular values can be used to determine the amount of variance $X^{\\prime}$ explains of the original data matrix $X$ when reducing the dimensions to find a lower rank approximation.\n",
    "\n",
    "\\begin{align}\n",
    "   X^{\\prime}_{k} &=  U_{k} \\, \\Sigma_{k} \\, V^{\\intercal}_{k} \\\\\n",
    "                           &\\approx X_{r} & \\text{ where } rank(X^{\\prime}) = k \\lt rank(X) = r\n",
    "\\end{align}\n",
    "\n",
    "The amount of variance that the reduced rank approximation $X^{\\prime}_{k}$ explains of $X_{r}$ is\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{cum. variance explained} &= \\frac{\\sum_{j=1}^{k} \\sigma_{j}^{2}}{\\sum_{i=1}^{r} \\sigma_{i}^{2}}\n",
    "\\end{align}\n",
    "\n",
    "**NOTE**: [`numpy.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) actually returns a $\\Sigma$ that is not a diagonal _matrix_, but a _list_ of the entries on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sv_iris = np.arange(1, S_iris.size+1)\n",
    "\n",
    "cum_var_explained_iris = [np.sum(np.square(S_iris[0:n])) / np.sum(np.square(S_iris)) for n in num_sv_iris]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the cumulative variance explained visually as a function of the number of singular values used when reducing rank to find a lower-ranked matrix $X^{\\prime}$ to approximate $X$. This will inform us as to how many dimensions we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7.0,5.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(num_sv_iris,\n",
    "         cum_var_explained_iris,\n",
    "         color='#2171b5',\n",
    "         label='variance explained',\n",
    "         alpha=0.65,\n",
    "         zorder=1000)\n",
    "\n",
    "plt.scatter(num_sv_iris,\n",
    "            sklearn.preprocessing.normalize(S_iris.reshape((1,-1))),\n",
    "            color='#fc4e2a',\n",
    "            label='singular values (normalized)',\n",
    "            alpha=0.65,\n",
    "            zorder=1000)\n",
    "\n",
    "plt.legend(loc='center right', scatterpoints=1, fontsize=8)\n",
    "\n",
    "ax.set_xticks(num_sv_iris)\n",
    "ax.set_xlim(0.8, 4.1)\n",
    "ax.set_ylim(0.0, 1.1)\n",
    "ax.set_xlabel(r'Number of singular values used')\n",
    "ax.set_ylabel('Variance in data explained')\n",
    "ax.set_title('Iris dataset: cumulative variance explained & singular values',\n",
    "             fontsize=14,\n",
    "             y=1.03)\n",
    "\n",
    "ax.set_facecolor('0.98')\n",
    "\n",
    "plt.grid(alpha=0.8, zorder=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction\n",
    "\n",
    "Judging from the curve representing cumulative variance explained in the figure above, we can see that\n",
    "\n",
    "* with 1 singular value, about 96.5% of the variance of $X$ can be explained\n",
    "* with 2 singular values, that number goes up to approximately 99.8%\n",
    "\n",
    "Since graphing the Iris dataset in 1D wouldn't be all that interesting (just dots on a line segment), let's try using the first 2 singular values to represent the data on the $x$- and $y$-axes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_setosa = np.where(iris.target==0)[0]\n",
    "idx_versicolor = np.where(iris.target==1)[0]\n",
    "idx_virginica = np.where(iris.target==2)[0]\n",
    "\n",
    "setosa_x = U_iris[idx_setosa, 0]\n",
    "setosa_y = U_iris[idx_setosa, 1]\n",
    "\n",
    "versicolor_x = U_iris[idx_versicolor, 0]\n",
    "versicolor_y = U_iris[idx_versicolor, 1]\n",
    "\n",
    "virginica_x = U_iris[idx_virginica, 0]\n",
    "virginica_y = U_iris[idx_virginica, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use different marker shapes and colors to represent the three Iris species on our 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7.0,5.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(setosa_x,\n",
    "            setosa_y,\n",
    "            marker='o',\n",
    "            color='#66c2a5',\n",
    "            label='Iris-setosa',\n",
    "            zorder=1000)\n",
    "\n",
    "plt.scatter(versicolor_x,\n",
    "            versicolor_y,\n",
    "            marker='D',\n",
    "            color='#fc8d62',\n",
    "            label='Iris-versicolor',\n",
    "            zorder=1000)\n",
    "\n",
    "plt.scatter(virginica_x,\n",
    "            virginica_y,\n",
    "            marker='^',\n",
    "            color='#8da0cb',\n",
    "            label='Iris-virginica',\n",
    "            zorder=1000)\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints=1, fontsize=8)\n",
    "\n",
    "ax.set_xlabel(r'singular value $\\sigma_{1}$')\n",
    "ax.set_ylabel(r'singular value $\\sigma_{2}$')\n",
    "ax.set_title('2D plot of Iris dataset',\n",
    "             fontsize=14,\n",
    "             y=1.03)\n",
    "ax.set_facecolor('0.98')\n",
    "\n",
    "plt.grid(alpha=0.6, zorder=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There!\n",
    "\n",
    "Now that we are viewing the originally 4D data with 2 dimensions using the first 2 singular value columns of the $U$ left singular vectors matrix, we can see that there should be a very clear separation for the _Iris setosa_ class and the others. On the other hand, the demarcation between _Iris versicolor_ and _Iris virginica_ might not be as clear cut.\n",
    "\n",
    "Nevertheless, since this 2D reduced-rank matrix representation $X^{\\prime}$ explains nearly 99.8% of the variance in the original dataset, we can be pretty certain that clustering and classification should be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Countries and Primary Languages\n",
    "\n",
    "### Or, What to do when your data is categorical?\n",
    "\n",
    "In the previous example, we were exploring the Iris dataset which is a matrix $X \\in \\mathbb{R}^{150 \\times 4}$. Singular value decomposition helped us to find a reduced-rank matrix $X^{\\prime} \\in \\mathbb{R}^{150 \\times 2}$ that accurately approximated the original data matrix and let us visualize the 4-dimensional data using 2 dimensions.\n",
    "\n",
    "Let's now consider another dataset where the values are not in $\\mathbb{R}$, but are _categorical_.\n",
    "\n",
    "For this example, we explore a fictional survey of 1000 respondents from each of five countries (Canada, USA, England, Italy and Switzerland), asking them what their primary language is (among English, French, Spanish, German and Italian). So in our data we have categories for both _country_ and _language_.\n",
    "\n",
    "We read in the data from file using [`pandas.dataframe.read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = os.path.join(os.getcwd(), \n",
    "                   'data', \n",
    "                   'country_language.csv')\n",
    "\n",
    "df_countries = pd.read_csv(fin, dtype='category')\n",
    "\n",
    "print(\"data has {} measurements for {} variables\\n\".format(*df_countries.shape))\n",
    "\n",
    "print(\"Here are the first 10 rows...\\n\\n{}\\n...\".format(df_countries.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correspondence Analysis\n",
    "\n",
    "#### Contingency table $F$\n",
    "\n",
    "Our next step in exploring the data is to break out the data in terms of the 2 categories.\n",
    "\n",
    "Here we convert the raw observations into a [contingency table](http://onlinestatbook.com/2/chi_square/contingency.html) $F$, with the countries as rows and the languages as columns. [`pandas.crosstab`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html) will do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Canada', 'USA', 'England', 'Italy', 'Switzerland']\n",
    "languages = ['English', 'French', 'Spanish', 'German', 'Italian']\n",
    "\n",
    "F = pd.crosstab(df_countries.country, df_countries.language, margins=False)\n",
    "F.index = countries\n",
    "F.columns = languages\n",
    "\n",
    "print(\"{}\".format(F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say that we are interested in seeing the relation between the countries and the languages.\n",
    "\n",
    "However, we cannot blindly apply singular value decomposition to contingency table $F$ above.\n",
    "\n",
    "Since we are working with 2 distinct categories, we can apply [correspondence analysis](http://www.mathematica-journal.com/2010/09/an-introduction-to-correspondence-analysis/) to transform the contingency table into a form where we can use singular value decomposition. At that point, we should be able to find a reduced-rank matrix that approximates the original data, and that in turn would let us graphically represent the the relations beween countries and languages.\n",
    "\n",
    "The idea is to use the $\\chi^{2}$ distance between rows and columns as our basis for singular value decomposition, as the [$\\chi^{2}$ distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) lets us calculate the independence of qualitative variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correspondence matrix $P$\n",
    "\n",
    "We start by first calculating the correspondence matrix $P$ where\n",
    "\n",
    "\\begin{align}\n",
    "  P &= \\left[ \\frac{f_{ij}}{\\sum_{i=1}^{I} \\sum_{j=1}^{J} f_{ij}} \\right] \\text{ for } f_{ij} \\in F\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = F / F.sum().sum()\n",
    "\n",
    "print('correspondence matrix P:\\n\\n{}'.format(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row centroid $p_{i+}$\n",
    "\n",
    "Using correspondence matrix $P$, we next obtain the _row centroid_ $p_{i+}$. The row centroid can be interpreted as the marginal frequency distribution over the sum of the countries (rows), and reflects the fact that there were equally 1000 respondents per country in our fictional study.\n",
    "\n",
    "The row centroid $p_{i+}$ is derived from correspondence matrix $P$ with\n",
    "\n",
    "\\begin{align}\n",
    "  p_{i+} &= \\sum_{j=1}^{J} p_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_centroid = P.sum(axis=1)\n",
    "\n",
    "print('row centroid (marginal frequency distribution over countries):\\n\\n{}'.format(row_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column centroid $p_{+j}$\n",
    "\n",
    "Similarly, we obtain the _column centroid_ $p_{+j}$ from $P$. The column centroid can be interpreted as the marginal frequency distribution over the sum of the languages (columns).\n",
    "\n",
    "The column centroid $p_{+j}$ is derived from correspondence matrix $P$ with\n",
    "\n",
    "\\begin{align}\n",
    "  p_{+j} &= \\sum_{i=1}^{I} p_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_centroid = P.sum(axis=0)\n",
    "\n",
    "print('column centroid (marginal frequency distribution over languages):\\n\\n{}'.format(col_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\chi^{2}$ distances between countries and languages\n",
    "\n",
    "So rather than using the contingency table $F$ as the basis for singular value decomposition, we can look at the $\\chi^{2}$ distances between rows and columns for visualizing the relation between countries and languages.\n",
    "\n",
    "The $\\chi^{2}$ statistic is given by\n",
    "\n",
    "\\begin{align}\n",
    "  \\chi^{2} &= \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\frac{(O_{ij} - E_{ij})^{2}}{E_{ij}} \\\\\n",
    "           &= N \\, \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\frac{(p_{ij} - \\mu_{ij})^{2}}{\\mu_{ij}} \\\\\n",
    "           &= N \\, \\Lambda^{2} \\\\\n",
    "           \\\\\n",
    "  \\Rightarrow \\Lambda^{2} &= \\frac{\\chi^{2}}{N} \\\\\n",
    "   \\Lambda &= \\left[  \\frac{p_{ij} - \\mu_{ij}}{\\sqrt{\\mu_{ij}}} \\right] = \\left[  \\frac{p_{ij} - p_{i+}\\,p_{+j}}{\\sqrt{p_{i+}\\,p_{+j}}} \\right]\n",
    "\\end{align}\n",
    "\n",
    "Inertia matrix $\\Lambda$ measures the distribution of the individual profiles (rows/columns) around the average profile (centroids). Thus inertia represents the observed deviation from independence.\n",
    "\n",
    "Through its relation with the statistic $\\chi^{2}$, inertia $\\Lambda$ (a matrix of _standardized residuals_) provides the basis for using singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mu_ij = row_centroid.values.reshape((P.index.size,1)) * col_centroid.values.reshape((1,P.columns.size))\n",
    "\n",
    "Lambda = (P - Mu_ij) / np.sqrt(Mu_ij)\n",
    "\n",
    "print('inertia Lambda:\\n\\n{}'.format(Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factorizing inertia matrix $\\Lambda$ with singular value decomposition\n",
    "\n",
    "Now that we have transformed contingency table $F$ into inertia matrix $\\Lambda$ where element $\\lambda_{ij} \\in \\mathbb{R}$, we can use singular value decomposition to factorize $\\Lambda$ instead of the original raw data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_lambda, S_lambda, Vt_lambda = np.linalg.svd(Lambda)\n",
    "\n",
    "num_sv_lambda = np.arange(1, S_lambda.size+1)\n",
    "\n",
    "cum_var_explained_lambda = [np.sum(np.square(S_lambda[0:n])) / np.sum(np.square(S_lambda)) for n in num_sv_lambda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we look at the cumulative variance explained visually as a function of the number of singular values used when reducing rank to find a lower-ranked inertia matrix $\\Lambda^{\\prime}$ to approximate $\\Lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7.0, 5.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(num_sv_lambda,\n",
    "         cum_var_explained_lambda,\n",
    "         color='#2171b5',\n",
    "         label='variance explained',\n",
    "         alpha=0.65,\n",
    "         zorder=1000)\n",
    "\n",
    "plt.scatter(num_sv_lambda,\n",
    "            sklearn.preprocessing.normalize(S_lambda.reshape((1,-1))),\n",
    "            color='#fc4e2a',\n",
    "            label='singular values (normalized)',\n",
    "            alpha=0.65,\n",
    "            zorder=1000)\n",
    "\n",
    "plt.legend(loc='lower left', scatterpoints=1, fontsize=8)\n",
    "\n",
    "ax.set_xticks(num_sv_lambda)\n",
    "ax.set_xlim(0.9, 5.1)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlabel(r'Number of singular values used')\n",
    "ax.set_ylabel('Variance in data explained')\n",
    "ax.set_title('Countries/languages dataset: cumulative var. explained & singular values',\n",
    "             fontsize=14,\n",
    "             y=1.03)\n",
    "\n",
    "ax.set_facecolor('0.98')\n",
    "\n",
    "plt.grid(alpha=0.8, zorder=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction\n",
    "\n",
    "Judging from the curve representing cumulative variance explained with respect to the number of singular values used, we see that\n",
    "\n",
    "* with 1 singular value, about 50.6% of the variance of inertia matrix $\\Lambda$ can be explained\n",
    "* with 2 singular values, that number goes up to 91.6%\n",
    "* with 3 singular values, we have 98.7%\n",
    "\n",
    "To mix things up a bit, let's try visualizing the countries and languages in 3D.\n",
    "\n",
    "For countries, we will take the first 3 columns of $U$ as the $x$-, $y$- and $z$-coordinates.\n",
    "\n",
    "But since `numpy.linalg.svd` returns $V^{\\intercal}$ instead of $V$, we will take the first 3 _rows_ of $V^{\\intercal}$ for the $x$-, $y$- and $z$-coordinates for the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_x = U_lambda[:, 0]\n",
    "country_y = U_lambda[:, 1]\n",
    "country_z = U_lambda[:, 2]\n",
    "\n",
    "lang_x = Vt_lambda[0, :]\n",
    "lang_y = Vt_lambda[1, :]\n",
    "lang_z = Vt_lambda[2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the relation between countries and languages\n",
    "\n",
    "That was a bit of work moving from raw data to contingency table, to correspondence matrix, to the inertia matrix, and then finally to singular value decomposition, but we are now ready to see how the categories of country and language relate to one another in 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D, proj3d\n",
    "\n",
    "fig = pylab.figure(figsize=(7.5,5.5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(country_x, country_y, country_z, marker='s', s=50, c='#2171b5')\n",
    "cntry_labels = []\n",
    "for i,(x,y,z) in enumerate(zip(country_x, country_y, country_z)):\n",
    "    x2, y2, _ = proj3d.proj_transform(x,y,z, ax.get_proj())\n",
    "    \n",
    "    label = pylab.annotate(Lambda.index[i],\n",
    "                          xy=(x2,y2),\n",
    "                          xytext=(-2,2),\n",
    "                          textcoords='offset points',\n",
    "                          ha='right',\n",
    "                          va='bottom',\n",
    "                          color='#2171b5',\n",
    "                          alpha=0.9)\n",
    "    cntry_labels.append(label)\n",
    "\n",
    "ax.scatter(lang_x, lang_y, lang_z, marker='o', s=50, c='#fc4e2a')\n",
    "lang_labels = []\n",
    "for i,(x,y,z) in enumerate(zip(lang_x, lang_y, lang_z)):\n",
    "    x2, y2, _ = proj3d.proj_transform(x,y,z, ax.get_proj())\n",
    "    \n",
    "    label = pylab.annotate(Lambda.columns[i],\n",
    "                          xy=(x2,y2),\n",
    "                          xytext=(-2,2),\n",
    "                          textcoords='offset points',\n",
    "                          ha='right',\n",
    "                          va='bottom',\n",
    "                          color='#fc4e2a',\n",
    "                          alpha=0.4)\n",
    "    lang_labels.append(label)\n",
    "\n",
    "def update_position(e):\n",
    "    for i,(x,y,z) in enumerate(zip(country_x, country_y, country_z)):\n",
    "        x2, y2, _ = proj3d.proj_transform(x,y,z, ax.get_proj())\n",
    "        cntry_labels[i].xy = x2, y2\n",
    "    for i,(x,y,z) in enumerate(zip(lang_x, lang_y, lang_z)):\n",
    "        x2, y2, _ = proj3d.proj_transform(x,y,z, ax.get_proj())\n",
    "        lang_labels[i].xy = x2, y2\n",
    "    fig.canvas.draw()\n",
    "fig.canvas.mpl_connect('button_release_event', update_position)\n",
    "\n",
    "ax.set_xlabel(r'singular value $\\sigma_{1}$')\n",
    "ax.set_xticks([-0.5, 0.0, 0.5])\n",
    "ax.set_ylabel(r'singular value $\\sigma_{2}$')\n",
    "ax.set_yticks([-0.5, 0.0, 0.4])\n",
    "ax.set_zlabel(r'singular value $\\sigma_{3}$')\n",
    "ax.set_zticks([-0.5, 0.0, 0.5])\n",
    "ax.set_title('3D plot of Countries/Languages dataset',\n",
    "             fontsize=16,\n",
    "             y=1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we are!\n",
    "\n",
    "You can see how the anglophone countries Canada, USA and England are in close proximity of English and with each other, with Spanish being close to the USA while French is closer to Canada. German is close to Switzerland, with French somewhat in proximity. Italian, however, is out close to Italy, both being located largely in isolation from the other countries and languages.\n",
    "\n",
    "This should match up with your intuition from contingency table $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Resources\n",
    "\n",
    "* [Making sense of principal component analysis, eigenvectors & eigenvalues](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)\n",
    "* [Correspondence analysis is a useful tool to uncover the relationships among categorical variables](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3718710/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A\n",
    "\n",
    "## PCA and SVD\n",
    "\n",
    "Principal components analysis (PCA) and singular value decomposition are closely related, and you may often hear both these terms used in the same breath.  \n",
    "\n",
    "Here is a quick mathematical treatment explaining how PCA and SVD are related.\n",
    "\n",
    "Consider data matrix $X \\in \\mathbb{R}^{m \\times n}$ where $m > n$, and all $x_{ij}$ are centered about the column means. With principal components analysis, we have\n",
    "\n",
    "\\begin{align}\n",
    "  \\text{covariance matrix } C &= \\frac{1}{m} \\, X^{\\intercal} \\, X & \\text{from PCA} \\\\\n",
    "  &= \\frac{1}{m} \\, V \\, \\Gamma \\, V^{\\intercal} & \\text{by eigendecomposition of } X^{\\intercal} \\, X \\\\\n",
    "  \\\\\n",
    "  \\text{ but } X &= U \\, \\Sigma V^{\\intercal} & \\text{from SVD} \\\\\n",
    "  \\\\\n",
    "  \\Rightarrow C &= \\frac{1}{m} \\, V \\, \\Sigma \\, U^{\\intercal} \\, U \\, \\Sigma V^{\\intercal} \\\\\n",
    "  &= \\frac{1}{m} \\, V \\, \\Sigma^{2} \\, V^{\\intercal} \\\\\n",
    "\\end{align}\n",
    "\n",
    "So we see that:\n",
    "\n",
    "1. the singular values in $\\Sigma$ obtained via SVD are really just the square roots of the eigenvalues in $\\Gamma$ of the covariance matrix in PCA.\n",
    "1. if you mean-center your raw data matrix $X$ and then calculate SVD, you are doing the same thing as PCA.\n",
    "1. the above example shows covariance of $X$ with respect to its columns ($X^{\\intercal} \\, X$); it also applies for covariance of $X$ with respect to rows ($X \\, X^{\\intercal}$).\n",
    "\n",
    "#### Iris dataset: PCA & SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(iris.data)\n",
    "\n",
    "# don't forget to mean-center the data before SVD\n",
    "X = iris.data - np.mean(iris.data, axis=0)\n",
    "U, S, Vt = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the eigenvalues of $\\Gamma$ derived from PCA with the singular values from $\\Sigma$ derived with SVD: $\\Gamma = \\Sigma^{2}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_pca = pca.get_covariance()\n",
    "\n",
    "print('eigenvalues from PCA:\\n{}\\n'.format(np.linalg.eigvals(C_pca * X.shape[0])))\n",
    "\n",
    "print('squared singular values from SVD:\\n{}'.format(np.square(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we obtain the covariance matrix $C$ derived with PCA, but using  $\\frac{1}{m} \\, V \\, \\Sigma^{2} \\, V^{\\intercal}$ from SVD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('covariance matrix C derived from PCA:\\n{}\\n'.format(C_pca))\n",
    "\n",
    "C_svd = (1. / X.shape[0]) * Vt.T.dot(np.diag(np.square(S))).dot(Vt) \n",
    "print('covariance matrix using S and Vt from SVD:\\n{}\\n'.format(C_svd))\n",
    "\n",
    "print('Are these matrices equivalent (element-wise closeness comparison)?\\n{}'.format(np.isclose(C_pca, C_svd)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
