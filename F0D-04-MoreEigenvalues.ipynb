{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (00): Data Science \n",
    "**(Module 03: Linear Algebra)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use,but NOT allowed to change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2022 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n",
    "## Session 12 MoreEigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues - going beyond the power method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 16\n",
    "rcParams['figure.figsize'] = (12,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants on the power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The power method revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in computing the eigenvalues (and vectors) of a\n",
    "general matrix, which may be very large.\n",
    "\n",
    "The power method gave the largest eigenvalue, in absolute magnitude, as\n",
    "long as it is unique and the eigenvectors are independent. It did this\n",
    "by constructing a sequence, multiplying each time by the matrix $A$ and\n",
    "normalizing.\n",
    "\n",
    "This is a very simple method, and when we only need the largest\n",
    "eigenvalue (e.g., for computing the spectral radius) gives exactly what\n",
    "we need.\n",
    "\n",
    "There may be times where we need different information. Provided it is\n",
    "still only one eigenvalue that we are trying to find, there are variants\n",
    "on the power method that can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. want to find the *smallest* eigenvalue. Important to find range of\n",
    "scales in problem – problems with wildly varying scales difficult to\n",
    "solve numerically.\n",
    "\n",
    "Use:\n",
    "\n",
    "$$\\lambda_i \\text{ are eigenvalues of } A \\Rightarrow\n",
    "    1/\\lambda_i \\text{ are eigenvalues of } A^{-1}$$\n",
    "\n",
    "So apply power method to inverse matrix:\n",
    "\n",
    "$$A {\\boldsymbol{x}}_{n+1} = {\\boldsymbol{x}}_n.$$\n",
    "\n",
    "Converges towards eigenvector whose eigenvalue has *minimum* modulus.\n",
    "Again, normalize at each step.\n",
    "\n",
    "Do *not* use $A^{-1}$ directly, but solve linear system; decomposition\n",
    "methods particularly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse power method example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix\n",
    "\n",
    "$$A =\n",
    "        \\begin{pmatrix}\n",
    "          1 & 2 & 3 \\\\\n",
    "          4 & 5 & 6 \\\\\n",
    "          7 & 8 & 0\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "has eigenvalues\n",
    "\n",
    "$$\\left\\{\n",
    "          \\begin{array}{c}\n",
    "            12.1229\\\\ -5.7345\\\\ -0.3884\n",
    "          \\end{array}\\right. .$$\n",
    "\n",
    "The inverse power method shows linear convergence towards\n",
    "$\\lambda = -0.3884$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_power_method(A, niterations_max=50, tol=1e-15):\n",
    "    xn = np.zeros((len(A), niterations_max+1))\n",
    "    xn[:, 0] = np.ones((len(A),)) + 1e-7*np.random.rand(len(A))\n",
    "    rn = np.ones((niterations_max+1,))\n",
    "    for k in range(niterations_max):\n",
    "        xn[:,k] = xn[:,k] / np.linalg.norm(xn[:,k])\n",
    "        xn[:,k+1] = np.linalg.solve(A, xn[:,k])\n",
    "        rn[k+1] = np.sum(xn[:,k+1])/np.sum(xn[:,k])\n",
    "        if (abs(rn[k+1]-rn[k]) < tol):\n",
    "            break\n",
    "    if k < niterations_max:\n",
    "        rn[k+2:] = rn[k+1] # This ensures the later values are set to something sensible.\n",
    "    return (1.0/rn[k+1], 1.0/rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,0.0]])\n",
    "lamda, v = np.linalg.eig(A)\n",
    "order = np.abs(lamda).argsort()\n",
    "lamda = lamda[order]\n",
    "lamda_power, lamda_seq = inverse_power_method(A)\n",
    "\n",
    "print(\"The minimum eigenvalue from the inverse power method is {} (exact is {}, error is {})\".format(lamda_power, lamda[0], \n",
    "                                                                                             abs(lamda_power - lamda[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.abs(lamda_seq - lamda[0])\n",
    "iterations = range(len(errors))\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(iterations[:20], errors[:20], 'kx')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel(r\"$\\|$ Error $\\|$\")\n",
    "ax.set_title(r\"Convergence of the inverse power method, $n=3$\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifted power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another minor variant allows us to find the eigenvalue closest to a\n",
    "given complex number $\\sigma$. We just have to make use of:\n",
    "\n",
    "$$\\lambda_i \\text{ are eigenvalues of } A \\Rightarrow\n",
    "    \\lambda_i - \\sigma \\text{ are eigenvalues of } A - \\sigma\n",
    "    \\text{Id}$$\n",
    "\n",
    "Therefore the smallest eigenvalue of $A - \\sigma \\text{Id}$ is the one\n",
    "closest to $\\sigma$; this is just an application of the inverse power\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifted power method example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix\n",
    "\n",
    "$$A =\n",
    "        \\begin{pmatrix}\n",
    "          1 & 2 & 3 \\\\\n",
    "          4 & 5 & 6 \\\\\n",
    "          7 & 8 & 0\n",
    "        \\end{pmatrix}$$\n",
    "\n",
    "has eigenvalues\n",
    "\n",
    "$$\\left\\{\n",
    "          \\begin{array}{c}\n",
    "            12.1229\\\\ -5.7345\\\\ -0.3884\n",
    "          \\end{array}\\right. .$$\n",
    "\n",
    "The shifted power method shows linear convergence to $\\lambda =\n",
    "      -5.7345$ for the eigenvalue closest to $-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_power_method(A, sigma, niterations_max=50, tol=1e-15):\n",
    "    Ashift = A - sigma * np.eye(len(A))\n",
    "    xn = np.zeros((len(A), niterations_max+1))\n",
    "    xn[:, 0] = np.ones((len(A),)) + 1e-7*np.random.rand(len(A))\n",
    "    rn = np.ones((niterations_max+1,))\n",
    "    for k in range(niterations_max):\n",
    "        xn[:,k] = xn[:,k] / np.linalg.norm(xn[:,k])\n",
    "        xn[:,k+1] = np.linalg.solve(Ashift, xn[:,k])\n",
    "        rn[k+1] = np.sum(xn[:,k+1])/np.sum(xn[:,k])\n",
    "        if (abs(rn[k+1]-rn[k]) < tol):\n",
    "            break\n",
    "    if k < niterations_max:\n",
    "        rn[k+2:] = rn[k+1] # This ensures the later values are set to something sensible.\n",
    "    return (1.0/rn[k+1] + sigma, 1.0/rn + sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_shift, lamda_seq = shifted_power_method(A, -5.0)\n",
    "\n",
    "print(\"The eigenvalue closest to -5.0 from the shifted power method is {} (exact is {}, error is {})\".format(lamda_shift, lamda[1], \n",
    "                                                                                             abs(lamda_shift - lamda[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.abs(lamda_seq - lamda[1])\n",
    "iterations = range(len(errors))\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(iterations[:30], errors[:30], 'kx')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel(r\"$\\|$ Error $\\|$\")\n",
    "ax.set_title(r\"Convergence of the shifted power method, $n=3$\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard trick: convert to equivalent problem that is easy to solve.\n",
    "Typical simpler problems based on diagonal or triangular matrices. To\n",
    "compute *all* the eigenvalues of $A$, note that for diagonal and\n",
    "triangular matrices, *the eigenvalues are the diagonal entries*.\n",
    "\n",
    "So which matrix problems are “equivalent”? Means the spectrum, the set\n",
    "of all eigenvalues, is unchanged. One particular useful case is the\n",
    "*similarity transform*:\n",
    "\n",
    "**Definition**: The matrices $A,B$ are similar if there exists a nonsingular matrix\n",
    "$P$ such that\n",
    "\n",
    "$$B = P A P^{-1}.$$\n",
    "\n",
    "**Theorem**: Similar matrices have the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric interpretation of similarity transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similarity transform expresses that the matrix equation\n",
    "\n",
    "$$A {\\boldsymbol{u}} = {\\boldsymbol{v}}$$\n",
    "\n",
    "is independent of the coordinates in which it is written.\n",
    "\n",
    "\n",
    "Under a change of coordinates that does not move the origin, the vectors\n",
    "“rotate” to e.g.\n",
    "\n",
    "$${\\boldsymbol{u}}' = R {\\boldsymbol{u}}$$\n",
    "\n",
    "for some rotation matrix $R$. Hence our original equation becomes\n",
    "\n",
    "$$R A R^{-1} {\\boldsymbol{u}}' = {\\boldsymbol{v}}';$$\n",
    "\n",
    "if the “physical” properties of $A$ and $R A R^{-1}$ are the same, then\n",
    "they are independent of the coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur’s theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to find a triangular matrix $B$ that $A$ is similar to; from\n",
    "that we can just read off the eigenvalues. Is this even possible?\n",
    "\n",
    "**Schur’s Theorem**: Every square matrix is unitary similar to a triangular matrix.\n",
    "\n",
    "**Corollary**: Every Hermitian matrix $A = A^{\\dagger}$ is unitary similar to a\n",
    "diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Householder reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schur’s Theorem**: Every square matrix is unitary similar to a triangular matrix.\n",
    "\n",
    "One key step in the proof of the theorem is the use of *Householder\n",
    "reflections*:\n",
    "\n",
    "**Lemma**: If ${\\boldsymbol{x}}$, ${\\boldsymbol{y}}$ are vectors such that\n",
    "$\\|{\\boldsymbol{x}}\\|_2 =\n",
    "  \\|{\\boldsymbol{y}}\\|_2$ and $({\\boldsymbol{x}},{\\boldsymbol{y}})$ is\n",
    "real then the matrix $U = \\text{Id} -\n",
    "  {\\boldsymbol{u}} {\\boldsymbol{u}}^{\\dagger}$ defined from\n",
    "\n",
    "$${\\boldsymbol{u}} = \\sqrt{2} \\frac{{\\boldsymbol{x}} - {\\boldsymbol{y}}}{\\|{\\boldsymbol{x}} - {\\boldsymbol{y}}\\|_2}$$\n",
    "\n",
    "gives $U {\\boldsymbol{x}} = {\\boldsymbol{y}}$.\n",
    "\n",
    "Geometric interpretation: to get ${\\boldsymbol{y}}$ from\n",
    "${\\boldsymbol{x}}$, reflect ${\\boldsymbol{x}}$ in plane orthogonal to\n",
    "${\\boldsymbol{u}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key steps in the proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of Schur’s theorem by induction $n$. Obviously true for $n=1$\n",
    "(scalar numbers). Aim: reduce matrix $A$ such that\n",
    "\n",
    "$$U A U^{\\dagger} = \\left(\n",
    "      \\begin{array}{c|c c c}\n",
    "        \\mu_1 & & {\\boldsymbol{w}}& \\\\ \\hline\n",
    "        0 & &  & \\\\\n",
    "        \\vdots & & A_{n-1}  & \\\\\n",
    "        0 & &  & \\\\\n",
    "      \\end{array}\\right).$$\n",
    "\n",
    "I.e., rotate first column into a scalar multiple of the unit column\n",
    "vector $\\hat{{\\boldsymbol{e}}}_1 = (1, 0, \\dots 0)^T$.\n",
    "\n",
    "Straightforward: construct appropriate Householder reflection\n",
    "\n",
    "$$U = \\text{Id} - {\\boldsymbol{u}}{\\boldsymbol{u}}^{\\dagger}.$$\n",
    "\n",
    "This rotates *to* $\\hat{{\\boldsymbol{e}}}_1$; but what should it rotate\n",
    "from?\n",
    "\n",
    "$$U A U^{\\dagger} = \\left(\n",
    "      \\begin{array}{c|c c c}\n",
    "        \\mu_1 & & {\\boldsymbol{w}}& \\\\ \\hline\n",
    "        0 & &  & \\\\\n",
    "        \\vdots & & A_{n-1}  & \\\\\n",
    "        0 & &  & \\\\\n",
    "      \\end{array}\\right).$$\n",
    "\n",
    "Want to rotate first column into scalar multiple of unit column vector\n",
    "$\\hat{{\\boldsymbol{e}}}_1 = (1, 0, \\dots 0)^T$.\n",
    "\n",
    "Construct the appropriate Householder reflection\n",
    "\n",
    "$$U = \\text{Id} - {\\boldsymbol{u}}{\\boldsymbol{u}}^{\\dagger}.$$\n",
    "\n",
    "This rotates *to* $\\hat{{\\boldsymbol{e}}}_1$; but what should it rotate\n",
    "from?\n",
    "\n",
    "New matrix has $\\hat{{\\boldsymbol{e}}}_1$ as eigenvector, eigenvalue\n",
    "$\\mu_1$. Must be using an eigenvector to construct rotation; (suitably\n",
    "scaled) eigenvector is used as the vector to rotate *from*.\n",
    "\n",
    "The appropriate Householder reflection matrix is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    U = \\text{Id} - {\\boldsymbol{u}}{\\boldsymbol{u}}^{\\dagger}, & \\mbox{} &  \n",
    "    {\\boldsymbol{u}} = \\sqrt{2}\\frac{\\left( \\hat{{\\boldsymbol{e}}}_1-\\hat{{\\boldsymbol{v}}}_1 \\right)\n",
    "    }{||\\hat{{\\boldsymbol{e}}}_1- \\hat{{\\boldsymbol{v}}}_1||_2},\n",
    "  \\end{aligned}$$\n",
    "\n",
    "where $\\hat{{\\boldsymbol{v}}}_1$ is the unit eigenvector with eigenvalue\n",
    "$\\mu_1$.\n",
    "\n",
    "We can verify this by computing the product\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    U A U^{\\dagger}\\hat{{\\boldsymbol{e}}}_1 =  \\mu_1 \\hat{{\\boldsymbol{e}}}_1   \n",
    "  \\end{aligned}$$\n",
    "\n",
    "for a general square matrix and\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\hat{{\\boldsymbol{e}}}_1^{\\dagger} U A U^{\\dagger} = \\mu_1 \\hat{{\\boldsymbol{e}}}_1^{\\dagger}\n",
    "  \\end{aligned}$$\n",
    "\n",
    "for an Hermitian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schur’s Theorem**: Every square matrix is unitary similar to a triangular matrix.\n",
    "\n",
    "**Corollary**: Every Hermitian matrix $A = A^{\\dagger}$ is unitary similar to a\n",
    "diagonal matrix.\n",
    "\n",
    "Does this help us?\n",
    "\n",
    "Although comforting, proof is useless in practice: it uses the\n",
    "eigenvectors, which is exactly the information we are trying to find.\n",
    "\n",
    "However, the steps used in the proof will be of fundamental importance\n",
    "in finding the full spectrum of a matrix as we will see next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-   The power method is an iterative procedure giving the eigenvalue of\n",
    "    largest absolute modulus and its associated eigenvector.\n",
    "\n",
    "-   By looking at the inverse matrix the power method can be modified to\n",
    "    give\n",
    "\n",
    "    1.  the eigenvalue of smallest absolute modulus (inverse power\n",
    "        method)\n",
    "\n",
    "    2.  the eigenvalue closest to a given $\\sigma \\in {\\mathbb C}$\n",
    "        (shifted inverse power method).\n",
    "\n",
    "-   To find the full spectrum it is easiest to transform the matrix to\n",
    "    an “equivalent” diagonal or triangular matrix; in these cases the\n",
    "    eigenvalues are the diagonal entries.\n",
    "\n",
    "-   Similar matrices $A$, $B = P A P^{-1}$ have the same spectrum.\n",
    "\n",
    "-   Schur’s theorem states that every square matrix is unitary similar\n",
    "    to a triangular matrix.\n",
    "\n",
    "-   A key step in the proof the the use of Householder reflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
