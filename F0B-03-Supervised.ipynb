{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (00): Data Science \n",
    "**(Module 01: Data Science)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use,but NOT allowed to change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2022 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4 Supervised Learning\n",
    "\n",
    "\n",
    "The purpose of this session is to demonstrate different coefficient and linear regression.\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "### Part 1 Data Dependency\n",
    "\n",
    "1.1 [Pearson's-r Correlation coefficient](#pearson)\n",
    "\n",
    "1.2 [Spearman's rank coefficient](#spearman)\n",
    "\n",
    "\n",
    "### Part 2 Linear Regression\n",
    "\n",
    "2.1 [Multiple Linear Regression](#mlr)\n",
    "\n",
    "2.2 [Regression for Median House Price](#rmhp)\n",
    "\n",
    "### Part 3 Distances\n",
    "\n",
    "3.1 [Euclidean Distance](#euclidean)\n",
    "\n",
    "3.2 [Cosine Distance](#cosine)\n",
    "\n",
    "3.3 [Term-by-Document Matrix](#t2d)\n",
    "\n",
    "### Part 4 K-NN Classification\n",
    "\n",
    "4.1 [K-NN in Python](#knn)\n",
    "\n",
    "4.2 [Decision Boundary](#db)\n",
    "\n",
    "### Part 5 Naive Bayes Classifier\n",
    "\n",
    "5.1 [NBC by Example](#nbc)\n",
    "\n",
    "5.2 [NBC Exercise](#nbc2)\n",
    "\n",
    "### Part 6 Confidence Interval\n",
    "\n",
    "6.1 [Population and Sample](#popsample)\n",
    "\n",
    "6.2 [Confidence Interval](#ci)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">1. Data Dependency</span>\n",
    "\n",
    "<a id = \"pearson\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.1 Pearson's-r Correlation coefficient</span>\n",
    "\n",
    "\n",
    "We assume $X=\\left\\{ X_{1},\\ldots,X_{n}\\right\\}$ \n",
    "and $Y=\\left\\{ Y_{1},\\ldots,Y_{n}\\right\\}$. Then Pearson-r correlation coefficient is defined as \n",
    "\n",
    "$$ \\rho(X,Y) = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\sigma_Y} =  \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i-\\bar{X})^2} \\sqrt{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}} $$\n",
    "\n",
    "Use the car data and find the Pearson's-r correlation coefficient between car weights and fuel consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "#The customized package in Matplotlib has a warning, but it does not affect its use. I will ignore this warning in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Data1/Auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miles = data['miles']\n",
    "weights = data['Weight']\n",
    "print(miles[:10])\n",
    "print(weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_r = np.cov(miles, weights)[0, 1] / (miles.std() * weights.std())\n",
    "print (pearson_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(miles,weights))\n",
    "#Calculate the correlation coefficient of the matrix, and return the matrix of the correlation coefficient of the Pearson product moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = data['Horse power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(weights,horse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=300)\n",
    "ax.scatter(weights,miles, alpha=0.7, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Car Weight (tons)')\n",
    "ax.set_ylabel('Miles Per Gallon')\n",
    "\n",
    "line_coef = np.polyfit(weights, miles, 1)\n",
    "print(\"The first-order fitting coefficient is:\",line_coef)\n",
    "line_fit_1 = np.poly1d(line_coef)\n",
    "print(\"The first-order polynomial is:\",line_fit_1)\n",
    "xx = np.arange(1, 5, 0.1)\n",
    "yy = line_coef[0]*xx + line_coef[1]\n",
    "\n",
    "ax.plot(xx, yy, 'r', lw=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: \n",
    "\n",
    "1. Find the Pearson's-r coefficient for two linearly dependent variables. Add some noise and see the effect of varying the noise. \n",
    "2. Simulate and visualize some data with positive linear correlation\n",
    "3. Simulate and visualize some data with negative linear correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(-5, 5, 0.1)\n",
    "pp = 1.5  # level of noise\n",
    "yy = xx + np.random.normal(0, pp, size=len(xx))\n",
    "# print(xx)\n",
    "# print(yy)\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='none')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "\n",
    "line_coef = np.polyfit(xx, yy, 1)\n",
    "# print(line_coef)\n",
    "line_xx = np.arange(-5, 5, 0.1)\n",
    "line_yy = line_coef[0]*line_xx + line_coef[1]\n",
    "\n",
    "ax.plot(line_xx, line_yy, 'b', lw=2)\n",
    "\n",
    "scipy.stats.pearsonr(xx, yy)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's r coefficient is limited to analyze the linear correlation between two variables. It is not capable to show the non-linear dependency. Investigate the Pearson's r coefficient between two variables that are correlated non-linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data, first for X\n",
    "xx = np.arange(-5, 5, 0.1)\n",
    "\n",
    "# assume Y = 2Y + some perturbation\n",
    "pp = 1.1  # level of noise\n",
    "yy = xx**2 + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='b')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "ax.set_title('$Y = X^2+\\epsilon$', size=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson's-r correlation is near zero which means there is no linear correlation. But how about non-linear correlation? Isn't $y=x^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(xx,yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"spearman\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.2 Spearman's rank coefficient</span>\n",
    "\n",
    "Spearman's rank coefficient is used for discrete/ordinal data. Find the Spearman's rank between horse power and number of cylinders of the car data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scipy.stats.spearmanr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horse = np.array([float(dd[4]) for dd in data[1:]])\n",
    "#cylinder = np.array([float(dd[2]) for dd in data[1:]])\n",
    "horse = data['Horse power']\n",
    "cylinder = data['cylinder number']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=300)\n",
    "ax.scatter(horse, cylinder, alpha=0.6, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Horse power')\n",
    "ax.set_ylabel('#Cylinders')\n",
    "\n",
    "print (scipy.stats.spearmanr(horse, cylinder))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**. \n",
    "Compute the spearman rank correlation between \"Horse power\" and \"Engine displacement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = data['Engine displacement']\n",
    "scipy.stats.spearmanr(horse,displacement)\n",
    "\n",
    "radius = 150*np.ones(len(horse))\n",
    "fig, ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "ax.scatter(displacement, horse, alpha=0.2, c='m', edgecolor='none',s=radius)\n",
    "\n",
    "ax.set_xlabel('Engine displacement')\n",
    "ax.set_ylabel('Horse power')\n",
    "plt.savefig(\"Engine_vs_horse.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">2. Linear Regression</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "#‚Äú%matplotlib inline‚ÄùÂ∞±ÊòØÊ®°‰ªøÂëΩ‰ª§Ë°åÊù•ËÆøÈóÆmagicÂáΩÊï∞ÁöÑÂú®IPython‰∏≠Áã¨ÊúâÁöÑÂΩ¢Âºè„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we investigate a simple case by fitting a linear regression for three data points. First we simulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating the data\n",
    "\n",
    "x = np.c_[0.0, 1.0, 2.0].T\n",
    "y  = [1, 1.5, 3.1]\n",
    "\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the data\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.set_title('simulated data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# instanciate the model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(x, y)\n",
    "yhat = lr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Coefficients:\", lr.coef_)\n",
    "print (\"Intercept:\", lr.intercept_)\n",
    "# print (\"Residues:\", lr.residues_)\n",
    "print(mean_squared_error(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the line to see how it estimates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.plot(x, yhat)\n",
    "\n",
    "ax.set_title('simulated data and the estimated line')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method `predict()` to predict `y` for a new `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.c_[4, 2.3].T\n",
    "y_test = lr.predict(x_test)\n",
    "\n",
    "print (x_test.T)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"mlr\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">2.1 Multiple Linear Regression</span>\n",
    "\n",
    "\n",
    "Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. For example if we have two explanatory variables (attributes, features), our data has such a form:\n",
    "\n",
    "$$\n",
    "D=\\left\\{ \\left(\\left(x_{1,1},x_{2,1}\\right),y_{1}\\right),\\left(\\left(x_{1,2},x_{2,2}\\right),y_{2}\\right),\\ldots,\\left(\\left(x_{1,n},x_{2,n}\\right),y_{n}\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "Now we fit a multiple linear regression $y = x_1 + 2x_2 + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the data\n",
    "\n",
    "x = np.c_[[0, 0], [0, 1], [1, 1], [1, 0]].T\n",
    "y = [1.5, 3.2, 4, 2]\n",
    "\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = linear_model.LinearRegression(fit_intercept=True)\n",
    "print(mlr.fit(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mlr.coef_)\n",
    "print (mlr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mlr.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(mlr.predict(x),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises 3**: \n",
    "\n",
    "As the score suggests, now we have the perfect regression. Change the values of $y$ slightly and see what effect it has on the `mlr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rmhp\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">2.2 Regression for median house prices</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the package `pandas` for reading and storing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Data2/housing_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scatter plot of the number of rooms vs the median house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7), dpi=300)\n",
    "median_prices = data['MEDV']\n",
    "avg_rooms = data['RM']\n",
    "scales = 50*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms, median_prices, color='b',s=scales, alpha=0.7, edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_rooms.shape)\n",
    "print(median_prices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correlated are the number of rooms and the price of the house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(avg_rooms, median_prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to fit a linear regression mode on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "x = np.c_[avg_rooms.values]\n",
    "y = median_prices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.fit(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lr.coef_)\n",
    "print (lr.intercept_)\n",
    "#print (lr.residues_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the model parameters\n",
    "\n",
    "print (lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "\n",
    "yhat = lr.predict(x)\n",
    "print (x[:10])\n",
    "print (yhat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the result\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "\n",
    "scales = 20*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms,median_prices,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "\n",
    "# plot the regression linear leared\n",
    "ax.plot(x,yhat)\n",
    "\n",
    "# visualize the residuals\n",
    "tmp = np.reshape(x,[1,len(x)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in range(len(x)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,y[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='g',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out the sum of residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(yhat,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is customary to test your model on **unseen** data. So we divide our data into two parts. We use 70% of it to train the model and 30% to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "split_idx = int(np.round(split * len(data)))\n",
    "print(split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0:200]\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "train_data.plot(kind='scatter', x='RM', y='MEDV', ax=axs[0], figsize=(7, 7))\n",
    "train_data.plot(kind='scatter', x='AGE', y='MEDV', ax=axs[1], figsize=(7, 7))\n",
    "#\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[200:300]\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data['RM'].values\n",
    "train_X = np.c_[train_X]\n",
    "train_Y = train_data['MEDV'].tolist()\n",
    "print(train_X)\n",
    "test_X = test_data['RM'].values\n",
    "\n",
    "test_X = np.c_[test_X]\n",
    "test_Y = test_data['MEDV'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(train_X))\n",
    "print (train_X.shape)\n",
    "print (type(train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build a linear regression model from training data\n",
    "'''\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "print(lr.fit(train_X, train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lr.coef_)\n",
    "print (lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the linear regression result and the data to see how it fits the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "# plot training data\n",
    "scales = 20*np.ones(len(train_Y))\n",
    "ax.scatter(train_X,train_Y,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "plt.title('Training a simple linear regression model')\n",
    "\n",
    "# plot the regression line\n",
    "train_Yhat = lr.predict(train_X)\n",
    "plt.plot(train_X,train_Yhat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the model parameters, we can use the model to predict for unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "# plot the predicted points along the prediction line\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,yhat_test,s=scales,color='b',edgecolor='r')\n",
    "ax.plot(test_X,yhat_test,color='b',linewidth=.2)\n",
    "\n",
    "# plot the true values\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,test_Y,s=scales,color='g',edgecolor='b')\n",
    "\n",
    "# plot the residual line\n",
    "tmp = np.reshape(test_X,[1,len(test_X)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in range(len(test_X)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat_test[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,test_Y[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='red',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">3. Distances</span>\n",
    "\n",
    "`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other, for example the distance between two vectors in an 2-dimensional space.\n",
    "\n",
    "Now that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id = \"euclidean\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.1 Euclidean Distance</span>\n",
    "\n",
    "Euclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n",
    "\n",
    "$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "We can use array operators for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 5, 4, 6, 8])\n",
    "x2 = np.array([3, 5, 6, 8, 6])\n",
    "\n",
    "print (x1 - x2)\n",
    "print ((x1 - x2) ** 2)\n",
    "print (np.sqrt(np.sum((x1 - x2) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance1(x1, x2):\n",
    "    d = x1 - x2\n",
    "    d = d ** 2\n",
    "    return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "\n",
    "print (euclidean_distance1(x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def euclidean_distance2(x1, x2):\n",
    "    if x1.shape[0] != x2.shape[0]:\n",
    "        sys.exit('x1 and x2 are not the same size')\n",
    "    else:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "print(euclidean_distance2(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance3(x1, x2):\n",
    "    try:\n",
    "        d = x1 - x2\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print(\"Vectors passed to the function are not the same size\")\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 2])\n",
    "a = euclidean_distance3(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance4(x1, x2):\n",
    "    try:\n",
    "        d = np.array(x1) - np.array(x2)\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print (\"Vectors passed to the function are not the same size\")\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"cosine\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.2 cosine similarity and distance</span>\n",
    "\n",
    "Cosine similarity is a measure of similarity between two vectors based on the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n",
    "\n",
    "$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n",
    "\n",
    "\n",
    "Cosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similarity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([3,4,6])\n",
    "\n",
    "print(x1 * x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2):\n",
    "    try:\n",
    "        num = (x1*x2).sum()\n",
    "        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n",
    "        num += 0.0    # or use np.astype(float) to make sure of float division\n",
    "        return 1 - num/denom\n",
    "    except ValueError as e:\n",
    "        print (\"Vectors passed to the function are not the same size\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "print(cosine_distance(x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"t2d\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.3 Term-by-Document matrix</span>\n",
    "\n",
    "A term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$.\n",
    "\n",
    "We will represent two datasets with term-by-document matrix:\n",
    "\n",
    "* a collection of 100 Twitter messages about Geelong\n",
    "* a collection of 6 news articles (5 about Apple and 1 about politics)\n",
    "\n",
    "The data is already collected and stores in text files. Thus you will need to:\n",
    "\n",
    "* read the text files\n",
    "    * using file object\n",
    "* perform pre-processing\n",
    "    * using string methods\n",
    "    * using re package\n",
    "* construct the term-by-document matrix\n",
    "    * using numpy arrays and operations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Twitter dataset\n",
    "First read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEELONG and beyond is the perfect FB page to keep you up to date on that area. Pop over and give them a LIKE too:\n",
      "\n",
      "https://t.co/hKvjPDsZKT\n",
      "\n",
      "CPSU members hungry for action at geelong trades hall http://t.co/gPAveTuSaV\n",
      "\n",
      "Club member Sharnee McIntosh and PT Anna Vines featured in this week's issue of the Geelong News.‚Ä¶ https://t.co/ddIuHsOwJv\n",
      "\n",
      "RONE Melbourne, Geelong, Australia. http://t.co/zrXyw7pvZu\n",
      "\n",
      "RT @CPSUnion: Geelong to be hit by fed PS strike today. #cpsusafeguard http://t.co/bgWEDw9zCa http://t.co/FBMh8rKaKQ\n",
      "\n",
      "RT @Tradeshallglg: CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "RT @geelongaddy: Shire to test water over pool cost: http://t.co/mgYg1O4j2b http://t.co/bAsid1QEXl\n",
      "\n",
      "Hey pool champions of @surfcoastnotes did you not know when you ran for election? Disgraceful! #surfcoastpool http://t.co/UohzWEsWaP\n",
      "\n",
      "Geelong Social is out! http://t.co/vsHhsHezgR Stories via @MarkBeyerle @M_Wills_Avenue @Himekami64\n",
      "\n",
      "Disgraceful work @surfcoastnotes stop this, make a commitment to your community and get on with it! http://t.co/m1tWeZWQ76\n",
      "\n",
      "RT @Tradeshallglg: CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "RT @AustMarConsSoc: Outrageous. How many more dolphins is the Geelong Star allowed to kill before it's banned from fishing in Australia? ht‚Ä¶\n",
      "\n",
      "@angusbooker Hopefully Geelong. Imagine being short high speed boat ride from dingy Docklands to a sweet bayside workplace in Geelong.\n",
      "\n",
      "newtownblonde large pom poms have landed at urbanstalker_geelong and #celine approves! #pompoms‚Ä¶ https://t.co/i9eOspkFzj\n",
      "\n",
      "#3Novices : ACCC eases GrainCorp, Emerald port rules http://t.co/cqc48x0siY Consumer watchdog reduces regulation on Melbourne, Geelong wh‚Ä¶\n",
      "\n",
      "RT @Tradeshallglg: CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "Last night was the second time I popped into the #BlueNote piano bar in downtown #Geelong - here is a‚Ä¶ https://t.co/FI66suyUJz\n",
      "\n",
      "Tammy painting Kevin's face from City of Greater Geelong before information forum. http://t.co/Cr6lBd3aCC\n",
      "\n",
      "RT @Tradeshallglg: CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "RT @Tradeshallglg: CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "RT @geelongaddy: Shire to test water over pool cost: http://t.co/mgYg1O4j2b http://t.co/bAsid1QEXl\n",
      "\n",
      "RT @Dr_Mel_Thomson: Thinking of Women in leadership in Geelong, I think of @comm4geelong Pls come to our event,  Key note: @Vic_LeadSci htt‚Ä¶\n",
      "\n",
      "AFL Round 12 - Melbourne defeats Geelong by 24 points[QUOTE=\"Partridge, post: 39260902, mem... http://t.co/SHF73oZ7CL\n",
      "\n",
      "RT @geelongaddy: LIVE FROM 10AM: Geelong public workers go on strike http://t.co/O7LHyH29Af http://t.co/w5uFrnDKEY\n",
      "\n",
      "Factory trawler Geelong Star operators Seafish Tasmania in workshop on how to ...   http://t.co/zCC4TJvXDN\n",
      "\n",
      "@AFL@Hungryforsport swans trade restrictions 2015 are a joke yet Geelong and Hwks can trade in and out who they like despite their success\n",
      "\n",
      "CPSU members meetingin geelong.  Standing room only http://t.co/SHUrGhixtJ\n",
      "\n",
      "Bryon (left) &amp; Tammy (right) with attendees of Wadawurrung information forum Geelong. @AusLandcare http://t.co/cOWVvsJomT\n",
      "\n",
      "RT @cassiezervos: CSIRO protesters strike at Trades Hall Geelong, demanding for their rights @geelongaddy http://t.co/bs8JxOHHIS\n",
      "\n",
      "@CottonOn it went from Lara to Sunshine to come back to Geelong. @auspost is so efficient.... üòê\n",
      "\n",
      "RT @GeelongBTigers: Bookings are open for the Geelong Tigers bus for the game Richmond V's GWS Giants at the MCG on Saturday 4th of... http‚Ä¶\n",
      "\n",
      "Walk for the Dogs ‚Äì Melbourne to Geelong 100kms | Sweet Shepherd Rescue Australia Inc http://t.co/DQpPanSSft\n",
      "\n",
      "RT @s_elliott79: @GreenestGreg @vline_geelong Regional Network Development Plan- book yourself into the Barwon South West session. http://t‚Ä¶\n",
      "\n",
      "RT @GreenestGreg: @s_elliott79 7:38 from Geelong was a 4 car N-set. 40% full at Lara, 95% full at Tarneit. 8min late @vline_geelong (grumbl‚Ä¶\n",
      "\n",
      "@JNorbs Disappointed to hear of you late arrivals. Expected better from #RRL @vline_geelong\n",
      "\n",
      "Rayban_Wholesale_52,Click_27 http://t.co/fjTChbmbY9 http://t.co/AtdZlXOalr @HOGGOLFBLOG @HOG_Geelong @HOLLYAMITCHELL @HOLLYCHICK1\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths - via @abcnews http://t.co/u7zKmiglIp\n",
      "\n",
      "@vline_geelong you've been operating for four days and you've already managed to cancel trains üòë\n",
      "\n",
      "@GreenestGreg @vline_geelong Regional Network Development Plan- book yourself into the Barwon South West session. http://t.co/8WSFDP1UHu\n",
      "\n",
      "@vline_geelong 2/4 morning trains I've caught this week have been cancelled. Unacceptable. People have places to be. #regionalfaillink\n",
      "\n",
      "Attend the free seminar WOMEN IN LEADERSHIP at our Geelong Waterfront Campus 12.30pm Fri 10th July #women #leaders http://t.co/VxUr6XWknM ‚Ä¶\n",
      "\n",
      "RT @GrowG21: #GROW will implement strategies to promote learning, engagement &amp; inclusion #GROWeducation #GROWrespect http://t.co/s34QSkk0RI\n",
      "\n",
      "Looking for a rewarding, flexible career move? We're expanding our service across greater Melbourne and Geelong!... http://t.co/EB6NTNBU9e\n",
      "\n",
      "RT @abcnewsAdelaide: Super trawler might breach SA fishing catch quota, Govt warns http://t.co/okMC4MwWfP #saparli #Adelaide http://t.co/D3‚Ä¶\n",
      "\n",
      "RT @Museontheloose: Geelong Social is out! http://t.co/RuSFxQSKdD Stories via @BellaGuria @LeButtery @Dulcerengel\n",
      "\n",
      "@s_elliott79 7:38 from Geelong was a 4 car N-set. 40% full at Lara, 95% full at Tarneit. 8min late @vline_geelong (grumble grumble)\n",
      "\n",
      "RT @TWRGeelong: In this week's edition catch up with the face of Brand Power, Sally Williams. http://t.co/SK7UW12YBP http://t.co/LBNiNXmeKQ\n",
      "\n",
      "RT @CPSUnion: Geelong to be hit by fed PS strike today. #cpsusafeguard http://t.co/bgWEDw9zCa http://t.co/FBMh8rKaKQ\n",
      "\n",
      "RT @CPSUnion: Geelong to be hit by fed PS strike today. #cpsusafeguard http://t.co/bgWEDw9zCa http://t.co/FBMh8rKaKQ\n",
      "\n",
      "Geelong day of action #cpsunion http://t.co/Ct7qlOx47m\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/WkzqZE1vYS\n",
      "\n",
      "In this week's edition catch up with the face of Brand Power, Sally Williams. http://t.co/SK7UW12YBP http://t.co/LBNiNXmeKQ\n",
      "\n",
      "@vline_geelong RRL not working well,buses replacing trains #cantorganiseacrumbinabakery\n",
      "\n",
      "@LMBR_50 Trawler Geelong Star's practices under spotlight after dolphin deaths: The operators of the controversial fishing trawler Ge...\n",
      "\n",
      "Just in: Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/NgwRgYfUvV #news\n",
      "\n",
      "RT @CPSUnion: Geelong to be hit by fed PS strike today. #cpsusafeguard http://t.co/bgWEDw9zCa http://t.co/FBMh8rKaKQ\n",
      "\n",
      "Lulay to play; could Lions QB be CFL‚Äôs most outstanding player? http://t.co/WencMWCXhE http://t.co/yGOjtZOGvV\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths: The operators of the controversial fishing‚Ä¶ http://t.co/V79Uuzzc40\n",
      "\n",
      "RT @geelongaddy: Rhys Stanley out for the season: http://t.co/Kh4S9m3wMK http://t.co/6vuUz2HaNv\n",
      "\n",
      "Rhys Stanley out for the season: http://t.co/Kh4S9m3wMK http://t.co/6vuUz2HaNv\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/dMZdeaMtZJ\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/CEfIFZjtj8\n",
      "\n",
      "RT @RichardMBaines: Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/PfBFoQLNg0 via @ABCNews #politas #aus‚Ä¶\n",
      "\n",
      "@Luke5SOS COME TO TORQUAY OR MY SCHOOL GEELONG LUTHERAN COLLEGE TODAY!!! ITS 1 &amp; 1/2 AWAY FROM MELBOURNE!!!! PRETTY PRETTY PLEASE!?!?!?\n",
      "\n",
      "Top geelong cats articles from last 48 hrs http://t.co/WunvbbEpzV\n",
      "\n",
      "Bookings are open for the Geelong Tigers bus for the game Richmond V's GWS Giants at the MCG on Saturday 4th of... http://t.co/Z9rlWVP7i2\n",
      "\n",
      "RT @Comm4Geelong: Interesting piece on CfG member @LtCreatures #Geelong in @TWRGeelong http://t.co/C0hcj8cDIi #Beer #BuyLocal http://t.co/X‚Ä¶\n",
      "\n",
      "CSIRO protesters strike at Trades Hall Geelong, demanding for their rights @geelongaddy http://t.co/bs8JxOHHIS\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/9znsrBYe4u\n",
      "\n",
      "Geelong Mission seeks to expand welcome to seafarers http://t.co/LwIpb6g2wR\n",
      "\n",
      "Interesting piece on CfG member @LtCreatures #Geelong in @TWRGeelong http://t.co/C0hcj8cDIi #Beer #BuyLocal http://t.co/XUL12OFrNA\n",
      "\n",
      "#AU Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/ED8krZ4KNK #abcnews\n",
      "\n",
      "#AU Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/Nplk1PeSGb #abcnews\n",
      "\n",
      "RT @geelongaddy: LIVE FROM 10AM: Geelong public workers go on strike http://t.co/O7LHyH29Af http://t.co/w5uFrnDKEY\n",
      "\n",
      "Commissions passage the sights but sounds pertinent to geelong, victoria: odAwed\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/PfBFoQLNg0 via @ABCNews #politas #auspol\n",
      "\n",
      "Geelong Organic is out! http://t.co/rGIXOqtRTM Stories via @vicenvironment @gogreen_natural @MylenePillot\n",
      "\n",
      "LIVE FROM 10AM: Geelong public workers go on strike http://t.co/O7LHyH29Af http://t.co/w5uFrnDKEY\n",
      "\n",
      "Centrelink, Medicare and child support services affected as Geelong public sector strike today @geelongaddy http://t.co/LfaeCgHmV6\n",
      "\n",
      "#Melbourne, #Geelong public sector workers protesting govt cuts with #cpsusafeguard actions. Help tell the story -&gt; http://t.co/JHcnsCPi4Y\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/WRmiIf9fdN\n",
      "\n",
      "Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/a7FMTHdGtb #abcnews\n",
      "\n",
      "#ABCNewsAus: Trawler Geelong Star's practices under spotlight after dolphin deaths http://t.co/kLvzy4xyWS #News_Monsta\n",
      "\n",
      "RT @CPSUnion: Geelong to be hit by fed PS strike today. #cpsusafeguard http://t.co/bgWEDw9zCa http://t.co/FBMh8rKaKQ\n",
      "\n",
      "Geelong Star operators to take workshop on how to avoid killing sea mammals http://t.co/96gCD9kKjm #geelongstar http://t.co/n1GHVpawyP\n",
      "\n",
      "@LMBR_50 Trawler Geelong Star's practices under spotlight after dolphin deaths: The operators of the controversial fishing trawler Ge...\n",
      "\n",
      "#Geelong, Australia: The Scarf Festival 2015 is now on at the National Wool Museum. http://t.co/5jaTTaYKl8 http://t.co/GYHYJACMOZ\n",
      "\n",
      "ABC: The operators of the controversial fishing trawler Geelong Star are having their practices scrutinised in... http://t.co/4CBzwROHMO\n",
      "\n",
      "And yet another nothing story from the Geelong addy on Land 400 and Sarah Henderson MP . http://t.co/ONOo34Jrzx\n",
      "\n",
      "Repost: I look at the happenings with Little Creatures Geelong, including White Rabbit and Furphy http://t.co/HHjDPxLLAr\n",
      "\n",
      "Centrelink, Medicare and child support services affected as Geelong public sector workers strike today: http://t.co/O7LHyGKyIH\n",
      "\n",
      "@MattyHeraldSun Geelong not available too?\n",
      "\n",
      "#TodayInHistory 26 June1857 Australias 1st country railway #Melbourne #Geelong opened to great fanfare http://t.co/Q3olNd31wW #history #rail\n",
      "\n",
      "How desperate for a page filler is the Geelong Addy with this drivel . http://t.co/gaiiJCgVoF\n",
      "\n",
      "Do you practice #yoga? http://t.co/Zf67ZGtksV #SeaireYoga offer private classes to get you started #Geelong #nourish #mindbodysoul\n",
      "\n",
      "Chicken processor Turi Foods drops labour-hire company over exploited worker claims at Geelong processing plant http://t.co/N2xvupsE5M\n",
      "\n",
      "Flying circus cabaret all set for Lyons‚Äô 50th bash ... http://t.co/YuKa0oftzc ‚Ä¶ http://t.co/1qpCfgaL3U\n",
      "\n",
      "RT @CLAN_AU: @manderson1944 Thanks heaps Margaret sent 20more #CLAN beanies #Geelong makes total160 woolenhats #Knitters http://t.co/xwz6P9‚Ä¶\n",
      "\n",
      "@nutwals Hi Peter, there was a track fault at Southern Cross and the train could not depart, and that formed the next train into Melbourne\n",
      "\n",
      "Geelong Star Factory Trawler Australian factory trawler kills nine dolphins - Echonetdaily: http://t.co/W2dDTf5wbW http://t.co/6heBQo1GjE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()   \n",
    "\n",
    "# join the subdirectory of the data and data file name\n",
    "file_path = os.path.join(cwd, \"Data/Data2/tweets.txt\")\n",
    "# read the contents of the file and store it in a list\n",
    "with open(file_path,encoding='utf-8') as fp:\n",
    "    tweets = fp.readlines()    \n",
    "for tweet in tweets:\n",
    "    print (tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first time it runs, an error will be reported: the file cannot be found in the current directory. \n",
    "#You need to create the corresponding folder under this address and put the file in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n",
    "\n",
    "In this case, our pre-processing consists of:\n",
    "\n",
    "* converting all the words into lower case to remove the effect of the letter case\n",
    "* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n",
    "* Removing the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "# import requests as r\n",
    "\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    # gettign rid of non ascii codes\n",
    "    doc = doc.encode('utf-8').decode('ascii', 'ignore',)\n",
    "    \n",
    "    # repalcing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Use the function provided to pre-process one of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termdoc(docs):\n",
    "    \"\"\"\n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()   \n",
    "    termdoc_sparse = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # pre-process the doc\n",
    "        doc_tokens = pre_process(doc)\n",
    "        # computes the frequencies for doc\n",
    "        doc_sparse = Counter(doc_tokens)    \n",
    "\n",
    "        termdoc_sparse.append(doc_sparse)\n",
    "\n",
    "        # update the vocab\n",
    "        vocab.update(doc_sparse.keys())  \n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "#     print(n_vocab)\n",
    "#     print(n_docs)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.items():\n",
    "            termdoc_dense[j, vocab.index(term)] = freq\n",
    "            \n",
    "    return termdoc_dense, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_termdoc, tweets_vocab = termdoc(tweets)\n",
    "print(tweets_termdoc)\n",
    "print(tweets_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at one of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j =0\n",
    "print (tweets[j])\n",
    "print (tweets_termdoc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_vocab.index('beyond'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_termdoc[j][127])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 News dataset\n",
    "Similar to previous sections, the data is stored in text files names as'news1.txt', ..., 'news5.txt'. All we have to do is read the files, construct the corpus and send it to `termdoc()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = 6\n",
    "cwd = os.getcwd()   \n",
    "news = []\n",
    "\n",
    "for j in range(1, n_docs+1):\n",
    "    filename = \"news{}.txt\".format(j)\n",
    "    file_path = os.path.join(cwd, \"Data/{}\".format(filename))\n",
    "    with open(file_path,encoding='utf-8') as fp:\n",
    "        news.append(fp.read())\n",
    "\n",
    "news_termdoc, news_vocab = termdoc(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now each news article is represented with a large vector of size `len(news_vocab)`. We can do many things with this representation. For example measuring the distance between two documents. The first 5 news articles are tech news and about Apple, but the 6th one is about politics. We expect that tech news be more similar to each other rather than to the politics news. In other words the distance between two articles from tech news should be less than the distance between a tech news article and a news article about politics. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_termdoc)\n",
    "type(news_termdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code works, but cosine_ There is an error in the value of distances output\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "print(cosine_distances(news_termdoc[1].reshape(-1, 1), news_termdoc[2].reshape(-1, 1)))\n",
    "print(cosine_distances(news_termdoc[1].reshape(-1, 1), news_termdoc[4].reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\">4. k-Nearest Neighbours Classification</span> \n",
    "\n",
    "kNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"knn\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">4.1 kNN in Python</span> \n",
    "\n",
    "To be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "n_per_class = 50\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "\n",
    "mean1 = [-5, 10]\n",
    "cov1 = [[1.5, 0], [0, 1.5]]\n",
    "mean2 = [0, 7]\n",
    "cov2 = [[1.5, 0], [0, 3]]\n",
    "mean3 = [-6, 6]\n",
    "cov3 = [[2, 0], [0, 1.5]]\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covs = [cov1, cov2, cov3]\n",
    "\n",
    "x11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\n",
    "x21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\n",
    "x31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n",
    "\n",
    "scale = 75\n",
    "alpha = 0.6\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(7, 7), dpi=300)\n",
    "ax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale)\n",
    "ax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale)\n",
    "ax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale)\n",
    "\n",
    "ax.set_title(\"synthesized data for 3 classes\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to instantiate a kNN classifier from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "weights='uniform'\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass one array as training features and on array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.r_[x11, x21, x31]\n",
    "x2 = np.r_[x12, x22, x32]\n",
    "X_train = np.c_[x1, x2]\n",
    "print (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.fit(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how kNN classifies a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k)\n",
    "print(knn.fit(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-7,10]\n",
    "Y_pred = knn.predict([X_test])\n",
    "#To change a one-dimensional array into a two-dimensional array, you only need to add a [] to the outer layer, and you can omit reshape\n",
    "\n",
    "ax.scatter(X_test[0], X_test[1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"db\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">4.2 Decision Boundry</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "# Create colour maps\n",
    "cmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n",
    "\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Prediction\n",
    "\n",
    "Play with the `X_test` and `k` to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-4, 8]\n",
    "Y_pred = knn.predict([X_test])\n",
    "ax.scatter(X_test[0], X_test[1], alpha=0.95, color='r', s=3*scale)\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n",
    "\n",
    "First we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "X1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\n",
    "Y_true = 0 * np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.c_[X1_test, X2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times the classifier predicts the labels correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred == Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sum(Y_pred == Y_true) + 0.0) / n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Repeat the previous experiment with a classifier which has been trained with a different `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use kNN to implement a classifier on handwritten digits dataset introduced in prac7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "Y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_validation import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.fit(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sum(Y_pred == Y_test) + 0.0) / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Y_pred != Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 1))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_test[mask][i, :].reshape(8, 8))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"{}\".format(Y_pred[mask][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">5. Naive Bayes Classifier</span> \n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc\"></a>\n",
    "### <span style=\"color:#0b486b\">5.1 NBC by Example</span> \n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"nb_data.png\" width=\"800\"> -->\n",
    "<img src=\"image/nb_data.png\" width=\"800\">\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: np=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'outlook': [0, 1, 2, 0, 1],\n",
    "    'temp'   : [0, 1, 2, 1, 0],\n",
    "    'humid'  : [0, 0, 1, 0, 1],\n",
    "    'wind'   : [0, 0, 1, 1, 0],\n",
    "    'play'   : [1, 1, 0, 0, 0,]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_prob(df, col):\n",
    "    ll = [(ss, (df[col] == ss).sum()) for ss in set(df[col])]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(df, f, c, val):\n",
    "    df2 = df[df[c] == val][f]\n",
    "    ll = [[ss, (df2 == ss).sum()] for ss in set(df2)]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print (\"probability of not playing: {}\".format(p0))\n",
    "print (\"probability of playing    : {}\".format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc2\"></a>\n",
    "### <span style=\"color:#0b486b\">5.2 NBC Exercise</span> \n",
    "\n",
    "\n",
    "\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    "    d1: Chinese Beijing Chinese , class = C\n",
    "    d2: Chinese Chinese Shanghai, class = C\n",
    "    d3: Chinese Macao           , class = C\n",
    "    d4: Tokyo Japan Chinese     , class = J\n",
    "\n",
    "\n",
    "Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    "    d5: Chinese Chinese Chinese Tokyo Japan, class = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">6. Confidence Interval</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"popsample\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">6.1 Population vs Sample</span> \n",
    "\n",
    "The main difference between population and sample comes down to how observations are assigned to dataset. A **population** includes all of the elements from the dataset. A **sample** consists of one or more observations from the population. In other words **population** is the entire collection of the desired measurable characteristic that we would have, if we could collect it. \n",
    "\n",
    "For example if suppose we want to find the average height of 2nd grade students in Australia. The population would be all the students who are studying in 2nd grade in Australia. But most probably we can not measure the height of all Australian 2nd grade students. It is not feasible. So what do we do? **We sample!**. We collect the height of some of Australian 2nd graders and based on that, we **estimate** the average height of the population (all of Australian 2nd grade students). The sample could be 2nd grade students of one class in one school, or multiple classes in multiple schools in one state, or  multiple schools in multiple states, or etc.\n",
    "\n",
    "A measurable statistic of a population (such as mean) is called a **parameter**. But a measurable characteristic of a sample is called **statistic**.\n",
    "\n",
    "----\n",
    "<a id = \"ci\"></a> \n",
    "\n",
    "### <span style=\"color:#0b486b\">6.2 Confidence Interval</span> \n",
    "\n",
    "As stated in previous section, population parameter is unknown. Confidence interval is a type of interval estimate of a population parameter calculated from sample statistics. It is an interval estimate combined with a probability.\n",
    "\n",
    "For the aforementioned example, it means that without collecting the height of all Aussie 2nd graders, we can estimate the average height by collecting a sample and using the below formula:\n",
    "\n",
    "\n",
    "$$Confidence\\, Interval=\\bar{X}\\pm z\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "$s$ is the sample standard deviation, $n$ is the sample size, and $z$ is often read from a table.\n",
    "\n",
    "    confidence level (%)     z\n",
    "         \n",
    "            70              1.04 \n",
    "            75              1.15\n",
    "            80              1.28 \n",
    "            85              1.44 \n",
    "            90              1.645\n",
    "            92              1.75\n",
    "            95              1.96\n",
    "            96              2.05\n",
    "            98              2.33\n",
    "            99              2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use an example to clarify this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a population. Please note that we do not use the population in our computations. We use samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 10, 2\n",
    "n_population = 10000\n",
    "population = np.random.normal(mu, sigma, n_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(population, bins=25)\n",
    "ax.set_title(r\"population histogram, $\\mu={}$, $\\sigma={}$ \".format(mu, sigma))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample multiple times from this population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "sample_size = 500\n",
    "samples = np.zeros([n_trials, sample_size])\n",
    "sample_std = np.zeros(n_trials)\n",
    "sample_mean = np.zeros(n_trials)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    samples[i] = np.random.choice(population, size=sample_size, replace=False)\n",
    "    sample_mean[i] = samples[i].mean()\n",
    "    sample_std[i] = samples[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "se = sample_std[i] / np.sqrt(sample_size)\n",
    "dist = ss.distributions.norm(sample_mean[i], se)\n",
    "z = 1.96\n",
    "\n",
    "x = np.linspace(9.5, 10.5, 100)\n",
    "y = dist.pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "ax.hist(sample_mean, density=True, bins=20, label='sample means')\n",
    "ax.plot(x, y, label=\"$N(\\mu_{i},se)$\")\n",
    "ax.vlines(sample_mean[i] + z*se, 0, 5, label='$\\mu_i \\pm z\\sigma_i$')\n",
    "ax.vlines(sample_mean[i] - z*se, 0, 5)\n",
    "\n",
    "ax.set_title(\"distribution of sample statistics\\ni={}\".format(i))\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
